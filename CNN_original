import tensorflow as tf
from tensorflow import keras
import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# For the preprocessing we will need further packages

from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Conv1D,MaxPooling1D
from keras.layers.embeddings import Embedding
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D
from pickle import load
from keras.utils.vis_utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Embedding
from keras.layers.convolutional import AveragePooling1D
from keras.layers.merge import concatenate
from keras.layers import LSTM




# Import the data file
datapath="C:/Users/Regina Eckhardt/Documents/Uni/Masterarbeit/"
filepath=datapath+"spam-flat3.csv"
mydata=pd.read_csv(filepath)

#print(mydata.head(10))

# the text column contains the body of the email and we only want to analyze the body
# the missing entries NaN will be replaced with ignore 
mydata['text'] = mydata['text'].fillna("Ignore")
new_data = mydata['text'].dropna(axis = 0, how ='any') 
  
# comparing sizes of data frames 
print("Old data frame length:", len(mydata['text']), "\nNew data frame length:",  
       len(new_data), "\nNumber of rows with at least 1 NA value: ", 
       (len(mydata['text'])-len(new_data)))
# From the last output we can see that we do not have any NaN values - so we can still work with mydata

texts = mydata['text'].tolist()
print(texts[:5])

# we want to replace phishing vs. spam with integer values since this is easier to handle and easier to interpret
mydata.loc[mydata['phishing'].astype(str) == 'False','phishing'] = 0
mydata.loc[mydata['phishing'].astype(str) == 'True','phishing'] = 1

# we also want to save this as a list and check how much of our data is actually phishing
label = mydata['phishing'].astype(int).tolist()
sum = mydata['phishing'].sum()
print('sum is ',sum)

# "We want to remove the data that does not have a classification into phishing or not"
final_data = mydata[mydata['phishing'].isin([0,1])]

# Again we can see that there is nothing to remove so we can again use mydata without any restrictions

# We need to split our dataset into a training and a test set.
X = texts
y = label
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)

vocab_size = 10000
encoded_train_texts = [one_hot(t,vocab_size) for t in X_train]
encoded_test_texts = [one_hot(t,vocab_size) for t in X_test]
    # "The way we train RNN's is to feed them a serie of sequences. The RNN's have well known issues with backpropagation of the gradient. This is the reason why we usually feed limited sequences to the RNN to train it. So the text should be cut into smaller pieces in order to build the training set.
    # "Keras only accepts sequences of the same length in a batch (Recurrent Models with sequences of mixed length). Therefore, pad_sequence is applied. 
    # "pad_sequence takes a LIST of sequences as an input (list of list) and will return a list of padded sequences. "
    # "# pad documents to a specific size
max_length = 70
padded_train_docs = pad_sequences(encoded_train_texts,maxlen=max_length,padding="post",truncating="post")
padded_test_docs = pad_sequences(encoded_test_texts,maxlen=max_length,padding="post",truncating="post")

inputs1 = Input(shape=(max_length,))
embedding1 = Embedding(vocab_size, 100)(inputs1)
conv1 = Conv1D(filters=48, kernel_size=4, activation='relu')(embedding1)
drop1 = Dropout(0.5)(conv1)
pool1 = MaxPooling1D(pool_size=2)(drop1)
flat1 = Flatten()(pool1)

inputs2 = Input(shape=(max_length,))
embedding2 = Embedding(vocab_size, 100)(inputs2)
conv2 = Conv1D(filters=48, kernel_size=6, activation='relu')(embedding2)
drop2 = Dropout(0.5)(conv2)
pool2 = MaxPooling1D(pool_size=2)(drop2)
flat2 = Flatten()(pool2)

inputs3 = Input(shape=(max_length,))
embedding3 = Embedding(vocab_size, 100)(inputs3)
conv3 = Conv1D(filters=48, kernel_size=8, activation='relu')(embedding3)
drop3 = Dropout(0.5)(conv3)
pool3 = MaxPooling1D(pool_size=2)(drop3)
flat3 = Flatten()(pool3)

merged = concatenate([flat1, flat2, flat3])
dense1 = Dense(10, activation='relu')(merged)
outputs = Dense(1, activation='sigmoid')(dense1)
model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs1)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit([trainX,trainX,trainX], array(y_train), validation_data=([testX,testX,testX],y_test), epochs=1, batch_size=16)

print(model.summary())
